{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 设定语言\n",
    "LANGS = [\n",
    "    'java',\n",
    "    'python',\n",
    "    'javascript',\n",
    "    'typescript',\n",
    "    'go'\n",
    "]\n",
    "\n",
    "# 创建目录\n",
    "NECESSARY_DIRS = [f'dataset/{x}' for x in LANGS] \\\n",
    "    + [f'enre_out/{x}' for x in LANGS] \\\n",
    "    + [f'repo/{x}' for x in LANGS]\n",
    "\n",
    "for d in NECESSARY_DIRS:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取各个数据集的仓库 URL，已有忽略\n",
    "\n",
    "# 根据使用修改：存放 CoEdPilot 训练集的目录\n",
    "base_folder = r\"C:\\Users\\aaa\\Desktop\\CoEdPilot-final-phase\\dataset&model\\locator_data\"\n",
    "\n",
    "def commit_url_to_repo_url(commit_url):\n",
    "    return re.search(r\"(.*?)/commit/\\w+$\", commit_url)[1]\n",
    "\n",
    "# 提取 base_folder 中 from_path 数据集的后缀为 lang_suffix 的文件的仓库地址，存放到 out_path\n",
    "def extract_distinct_repo_url(from_path, to_path, lang_suffix):\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, from_path), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".{lang_suffix}\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    distinct_repo_url = list(distinct_repo_url)\n",
    "\n",
    "    with open(to_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(distinct_repo_url, f)\n",
    "\n",
    "# 为指定 lang 提取仓库地址\n",
    "def extract_distinct_repo_url_for_lang(lang, lang_suffix):\n",
    "    extract_distinct_repo_url(f\"{lang}/test.jsonl\", f\"repo_url/{lang}_repo_urls.json\", lang_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_distinct_repo_url_for_lang(\"javascript\", \"js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行对应语言的 ENRE 程序，并制作数据集\n",
    "\n",
    "import make_nn_dataset_for_lang\n",
    "import importlib\n",
    "importlib.reload(make_nn_dataset_for_lang)\n",
    "import json\n",
    "\n",
    "# 为指定语言运行 make_dataset\n",
    "def make_dataset_for_lang(lang):\n",
    "    repos = []\n",
    "    with open(f\"repo_url/{lang}_repo_urls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        repos = json.load(f)\n",
    "\n",
    "    for repo_url in repos:\n",
    "        make_nn_dataset_for_lang.make_dataset(repo_url, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vercel/next.js. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" && node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\typescript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\next.js -v\"\n"
     ]
    }
   ],
   "source": [
    "make_nn_dataset_for_lang.make_dataset(\"https://github.com/vercel/next.js\", \"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vuejs/vue-cli. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\vue-cli -o vue-cli-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"vue-cli\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "Ignoring https://github.com/yarnpkg/yarn. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\yarn -o yarn-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"yarn\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\" && git clone \"https://github.com/vercel/next.js.git\"\"\n"
     ]
    }
   ],
   "source": [
    "make_dataset_for_lang(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from run import main\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "\n",
    "def combine_dataset_for_lang(lang):\n",
    "    dataset_dir = f\"dataset/{lang}\"\n",
    "    dataset_of_repos = [f.path for f in os.scandir(dataset_dir) if f.is_dir()]\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        all_data = []\n",
    "        for repo in dataset_of_repos:\n",
    "            with open(f\"{repo}/{dataset_type}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data += json.load(f)\n",
    "        with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(all_data, f)\n",
    "        \n",
    "def run_training_main(lang):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    batch_size = 32\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    main(model_name, lang, batch_size, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dataset_for_lang(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java, --train: True, --test True, --batch_size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/478853 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 478853/478853 [05:14<00:00, 1522.07it/s]\n",
      "100%|██████████| 68407/68407 [00:44<00:00, 1548.38it/s]\n",
      "100%|██████████| 136816/136816 [01:29<00:00, 1529.26it/s]\n",
      "Training: 100%|██████████| 14965/14965 [3:24:59<00:00,  1.22it/s, loss=1.01]    \n",
      "Training: 100%|██████████| 14965/14965 [3:18:42<00:00,  1.26it/s, loss=0.0333]  \n",
      "100%|██████████| 2138/2138 [08:24<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9698130308301782, precision: 0.964363916359874, recall: 0.985637696129064, F1: 0.9748847618005132\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9673132866519508, precision: 0.96370441922241, recall: 0.9819635826771653, F1: 0.9727483241925655\n",
      "Validation Loss: 0.10413794356012339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [16:48<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9695284177289206, precision: 0.9641039730511025, recall: 0.9857828588179254, F1: 0.9748229027641059\n",
      "mode: test, A depend by B\n",
      "acc: 0.9670140919190738, precision: 0.9636727791703146, recall: 0.9818661713671245, F1: 0.9726844089893898\n",
      "Test Loss: 0.10475712369907894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14965/14965 [3:00:18<00:00,  1.38it/s, loss=0.0292]   \n",
      "Training:  39%|███▉      | 5832/14965 [1:27:08<21:05:01,  8.31s/it, loss=0.115]  "
     ]
    }
   ],
   "source": [
    "run_training_main(\"java\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
