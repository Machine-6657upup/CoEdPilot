{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 设定语言\n",
    "LANGS = [\n",
    "    'java',\n",
    "    'python',\n",
    "    'javascript',\n",
    "    'typescript',\n",
    "    'go'\n",
    "]\n",
    "\n",
    "# 创建目录\n",
    "NECESSARY_DIRS = [f'dataset/{x}' for x in LANGS] \\\n",
    "    + [f'enre_out/{x}' for x in LANGS] \\\n",
    "    + [f'repo/{x}' for x in LANGS]\n",
    "\n",
    "for d in NECESSARY_DIRS:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取各个数据集的仓库 URL，已有忽略\n",
    "\n",
    "# 根据使用修改：存放 CoEdPilot 训练集的目录\n",
    "base_folder = r\"C:\\Users\\aaa\\Desktop\\CoEdPilot-final-phase\\dataset&model\\locator_data\"\n",
    "\n",
    "def commit_url_to_repo_url(commit_url):\n",
    "    return re.search(r\"(.*?)/commit/\\w+$\", commit_url)[1]\n",
    "\n",
    "# 提取 base_folder 中 from_path 数据集的后缀为 lang_suffix 的文件的仓库地址，存放到 out_path\n",
    "def extract_distinct_repo_url(from_path, to_path, lang_suffix):\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, from_path), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".{lang_suffix}\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    distinct_repo_url = list(distinct_repo_url)\n",
    "\n",
    "    with open(to_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(distinct_repo_url, f)\n",
    "\n",
    "# 为指定 lang 提取仓库地址\n",
    "def extract_distinct_repo_url_for_lang(lang, lang_suffix):\n",
    "    extract_distinct_repo_url(f\"{lang}/test.jsonl\", f\"repo_url/{lang}_repo_urls.json\", lang_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {}\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"dev\"]\n",
    "for i in DATASET_TYPES:\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, f\"java/{i}.jsonl\"), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".java\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    sets[i] = distinct_repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_distinct_repo_url_for_lang(\"\", \"js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行对应语言的 ENRE 程序，并制作数据集\n",
    "\n",
    "import make_nn_dataset_for_lang\n",
    "import importlib\n",
    "importlib.reload(make_nn_dataset_for_lang)\n",
    "import json\n",
    "\n",
    "# 为指定语言运行 make_dataset\n",
    "def make_dataset_for_lang(lang):\n",
    "    repos = []\n",
    "    with open(f\"repo_url/{lang}_repo_urls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        repos = json.load(f)\n",
    "\n",
    "    for repo_url in repos:\n",
    "        make_nn_dataset_for_lang.make_dataset(repo_url, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vercel/next.js. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" && node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\typescript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\next.js -v\"\n"
     ]
    }
   ],
   "source": [
    "make_nn_dataset_for_lang.make_dataset(\"\", \"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vuejs/vue-cli. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\vue-cli -o vue-cli-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"vue-cli\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "Ignoring https://github.com/yarnpkg/yarn. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\yarn -o yarn-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"yarn\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\" && git clone \"https://github.com/vercel/next.js.git\"\"\n"
     ]
    }
   ],
   "source": [
    "make_dataset_for_lang(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from run import train_on_single_lang, run_test\n",
    "from itertools import chain\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "\n",
    "def combine_dataset_for_lang(lang):\n",
    "    dataset_dir = f\"dataset/{lang}\"\n",
    "    dataset_of_repos = [f.path for f in os.scandir(dataset_dir) if f.is_dir()]\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        all_data = []\n",
    "        for repo in dataset_of_repos:\n",
    "            with open(f\"{repo}/{dataset_type}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data += json.load(f)\n",
    "        with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(all_data, f)\n",
    "\n",
    "def combine_dataset_for_multiple_lang(langs, out_dir, weights):\n",
    "    if weights is None:\n",
    "        weights = {}\n",
    "    for lang in langs:\n",
    "        if lang not in weights:\n",
    "            weights[lang] = 1.0\n",
    "    print(weights)\n",
    "    \n",
    "    all_data = {x:{y:[] for y in DATASET_TYPES} for x in langs}\n",
    "    \n",
    "    for lang in langs:\n",
    "        dataset_dir = f\"dataset/{lang}\"\n",
    "        for dataset_type in DATASET_TYPES:\n",
    "            data = all_data[lang][dataset_type]\n",
    "            with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                data_to_add = json.load(f)\n",
    "                if weights[lang] < 1.0:\n",
    "                    data_to_add = random.sample(data_to_add, math.floor(len(data_to_add) * weights[lang]))\n",
    "                data += data_to_add\n",
    "\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        data = list(chain.from_iterable([all_data[x][dataset_type] for x in langs]))\n",
    "        random.shuffle(data)\n",
    "        with open(os.path.join(out_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "def train_on_lang_default(lang):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    batch_size = 32\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    train_on_single_lang(model_name, lang, batch_size, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dataset_for_lang(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java, --train: True, --test True, --batch_size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/478853 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 478853/478853 [05:14<00:00, 1522.07it/s]\n",
      "100%|██████████| 68407/68407 [00:44<00:00, 1548.38it/s]\n",
      "100%|██████████| 136816/136816 [01:29<00:00, 1529.26it/s]\n",
      "Training: 100%|██████████| 14965/14965 [3:24:59<00:00,  1.22it/s, loss=1.01]    \n",
      "Training: 100%|██████████| 14965/14965 [3:18:42<00:00,  1.26it/s, loss=0.0333]  \n",
      "100%|██████████| 2138/2138 [08:24<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9698130308301782, precision: 0.964363916359874, recall: 0.985637696129064, F1: 0.9748847618005132\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9673132866519508, precision: 0.96370441922241, recall: 0.9819635826771653, F1: 0.9727483241925655\n",
      "Validation Loss: 0.10413794356012339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [16:48<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9695284177289206, precision: 0.9641039730511025, recall: 0.9857828588179254, F1: 0.9748229027641059\n",
      "mode: test, A depend by B\n",
      "acc: 0.9670140919190738, precision: 0.9636727791703146, recall: 0.9818661713671245, F1: 0.9726844089893898\n",
      "Test Loss: 0.10475712369907894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14965/14965 [3:00:18<00:00,  1.38it/s, loss=0.0292]   \n",
      "Training: 100%|██████████| 14965/14965 [7:06:37<00:00,  1.71s/it, loss=0.0116]   \n",
      "100%|██████████| 2138/2138 [18:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9746078617685324, precision: 0.9666482844606421, recall: 0.991490826816192, F1: 0.978911969308842\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9740669814492668, precision: 0.9660862474216914, recall: 0.9911417322834646, F1: 0.9784536157601963\n",
      "Validation Loss: 0.08887577650657358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [28:28<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9747178692550579, precision: 0.9668389216993714, recall: 0.9917677378378709, F1: 0.9791446848791429\n",
      "mode: test, A depend by B\n",
      "acc: 0.974045433282657, precision: 0.9663763419080413, recall: 0.991091939977516, F1: 0.97857810756191\n",
      "Test Loss: 0.08917729847261227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 156/14965 [04:53<7:44:48,  1.88s/it, loss=0.0111] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_training_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mrun_training_main\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     22\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --lang: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py:167\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(model_name, lang, batch_size, train, test)\u001b[0m\n\u001b[0;32m    164\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    165\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 167\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    169\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    173\u001b[0m     }, checkpoint_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_lang_default(\"java_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2269/25469 [00:00<00:08, 2676.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 25469/25469 [00:12<00:00, 2097.12it/s]\n",
      "100%|██████████| 796/796 [03:20<00:00,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.7414503906710118, precision: 0.5545750854598934, recall: 0.8766021613470721, F1: 0.6793592053367095\n",
      "mode: test, A depend by B\n",
      "acc: 0.5549491538733362, precision: 0.3475036179450072, recall: 0.4824814768303403, F1: 0.40401703559598295\n",
      "Test Loss: 1.5594558830387029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_test(\"model/java/model_checkpoint_java_23_12_24.bin\", \"python\", 'huggingface/CodeBERTa-small-v1', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1.0, 'java': 0.1}\n"
     ]
    }
   ],
   "source": [
    "combine_dataset_for_multiple_lang([\"python\", \"java\"], \"dataset/python_java\", {\"python\": 1.0, \"java\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java_small, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1607/143655 [00:00<01:00, 2349.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2800 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 143655/143655 [01:00<00:00, 2362.15it/s]\n",
      "100%|██████████| 20522/20522 [00:08<00:00, 2430.77it/s]\n",
      "100%|██████████| 41044/41044 [00:16<00:00, 2437.22it/s]\n",
      "Training: 100%|██████████| 8979/8979 [55:50<00:00,  2.68it/s, loss=0.222]  \n",
      "Training: 100%|██████████| 8979/8979 [55:37<00:00,  2.69it/s, loss=0.135]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9465451710359614, precision: 0.9523265306122449, recall: 0.9578783151326054, F1: 0.955094355069794\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9414774388461163, precision: 0.9486153092067642, recall: 0.9530531845042679, F1: 0.9508290685772774\n",
      "Validation Loss: 0.17346096512682255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:17<00:00,  2.92it/s, loss=0.0194]  \n",
      "Training: 100%|██████████| 8979/8979 [51:19<00:00,  2.92it/s, loss=0.0138]   \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9547802358444596, precision: 0.9612199721242929, recall: 0.9626406108875935, F1: 0.9619297669839186\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9520027287788715, precision: 0.9599934280785345, recall: 0.9591267235718975, F1: 0.9595598801165989\n",
      "Validation Loss: 0.14275865586723518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:19<00:00,  2.92it/s, loss=0.00176]  \n",
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.00417]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9609687164993665, precision: 0.955849358974359, recall: 0.9794728631250513, F1: 0.9675169309379943\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9589708605399084, precision: 0.9541159513132608, recall: 0.9779218647406435, F1: 0.9658722438391699\n",
      "Validation Loss: 0.14923144726442727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.0382]   \n",
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.00041]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9496637754604814, precision: 0.9387498031806015, recall: 0.9790623203875524, F1: 0.9584823761102849\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9481532014423546, precision: 0.9395256916996048, recall: 0.9754596191726855, F1: 0.9571555126036885\n",
      "Validation Loss: 0.19144804556077363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41044/41044 [00:17<00:00, 2406.59it/s]\n",
      "100%|██████████| 2566/2566 [05:06<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9523925543319365, precision: 0.9418713450292397, recall: 0.9811956786613597, F1: 0.9611314449395291\n",
      "mode: test, B depend by A\n",
      "acc: 0.950687067537277, precision: 0.9426489599247855, recall: 0.9772570361044551, F1: 0.9596410767696909\n",
      "Test Loss: 0.17644720438354622\n",
      "--model: huggingface/CodeBERTa-small-v1, --lang: python_java, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected checkpoint file: ./model/java_small\\model_java_small_20231225_225929.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1869/136995 [00:00<01:05, 2054.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 136995/136995 [01:00<00:00, 2261.34it/s]\n",
      "100%|██████████| 19571/19571 [00:08<00:00, 2289.92it/s]\n",
      "100%|██████████| 39150/39150 [00:16<00:00, 2307.88it/s]\n",
      "Training: 100%|██████████| 8563/8563 [53:02<00:00,  2.69it/s, loss=0.47]   \n",
      "Training: 100%|██████████| 8563/8563 [53:05<00:00,  2.69it/s, loss=0.128]  \n",
      "100%|██████████| 1224/1224 [02:26<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9508456389555976, precision: 0.9313440077632217, recall: 0.9509536784741145, F1: 0.9410466968991299\n",
      "mode: dev, B depend by A\n",
      "acc: 0.946349190128251, precision: 0.9151780432982373, recall: 0.9587309455942497, F1: 0.9364483718678126\n",
      "Validation Loss: 0.18471033477948773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8563/8563 [48:57<00:00,  2.91it/s, loss=0.00892] \n",
      "Training: 100%|██████████| 8563/8563 [49:00<00:00,  2.91it/s, loss=0.0244]   \n",
      "100%|██████████| 1224/1224 [02:26<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9597363445914874, precision: 0.9399758454106281, recall: 0.9639583849393114, F1: 0.9518160694631284\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9585100403658474, precision: 0.9366951498375256, recall: 0.9645557070268931, F1: 0.9504212968616437\n",
      "Validation Loss: 0.12643623364139334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8563/8563 [48:59<00:00,  2.91it/s, loss=0.000844] \n",
      "Training: 100%|██████████| 8563/8563 [50:56<00:00,  2.80it/s, loss=0.000804] \n",
      "100%|██████████| 1224/1224 [02:33<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9562107199427725, precision: 0.9348114230630197, recall: 0.9608620262571216, F1: 0.9476577291882978\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9549844157171324, precision: 0.9301101005265677, recall: 0.9631924649894659, F1: 0.9463622526636225\n",
      "Validation Loss: 0.1652732740799395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 793/8563 [04:43<46:21,  2.79it/s, loss=0.00348]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aaa\\AppData\\Local\\Temp\\ipykernel_31864\\2672801796.py\", line 9, in <module>\n",
      "    train_on_lang_from_checkpoint(\"python_java\", \"./model/java_small/*.bin\", epoch=8, batch_size=16)\n",
      "  File \"C:\\Users\\aaa\\AppData\\Local\\Temp\\ipykernel_31864\\2672801796.py\", line 6, in train_on_lang_from_checkpoint\n",
      "    train_on_single_lang(model_name, lang, batch_size, train, test, checkpoint_path, epoch)\n",
      "  File \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py\", line 174, in train_on_single_lang\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def train_on_lang_from_checkpoint(lang, checkpoint_path=None, epoch=10, batch_size=32):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    train_on_single_lang(model_name, lang, batch_size, train, test, checkpoint_path, epoch)\n",
    "\n",
    "train_on_lang_from_checkpoint(\"java_small\", epoch=8, batch_size=16)\n",
    "train_on_lang_from_checkpoint(\"python_java\", \"./model/java_small/*.bin\", epoch=8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java_small, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected checkpoint file: ./model/java\\model_checkpoint_java_23_12_24.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1656/143655 [00:00<00:59, 2404.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2800 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 143655/143655 [01:02<00:00, 2286.07it/s]\n",
      "100%|██████████| 20522/20522 [00:08<00:00, 2291.92it/s]\n",
      "100%|██████████| 41044/41044 [00:17<00:00, 2328.42it/s]\n",
      "Training:   1%|          | 67/8979 [00:29<1:04:27,  2.30it/s, loss=0.82] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_on_lang_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava_small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/java/*.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mtrain_on_lang_from_checkpoint\u001b[1;34m(lang, checkpoint_path, epoch, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --lang: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_on_single_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py:174\u001b[0m, in \u001b[0;36mtrain_on_single_lang\u001b[1;34m(model_name, lang, batch_size, train, test, checkpoint_path, epoch_num)\u001b[0m\n\u001b[0;32m    171\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    172\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 174\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    176\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    180\u001b[0m     }, save_checkpoint_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_lang_from_checkpoint(\"java_small\", \"./model/java/*.bin\", epoch=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/java/'\n",
    "for dataset_type in DATASET_TYPES:\n",
    "    with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_to_add = json.load(f)\n",
    "        data_to_add = random.sample(data_to_add, math.floor(len(data_to_add) * 0.3))\n",
    "        with open('dataset/java_small/' + dataset_type + '.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(data_to_add, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127310\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "tot_len = 0\n",
    "for i in DATASET_TYPES:\n",
    "    with open(f\"./dataset/python/{i}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        tot_len += len(json.load(f))\n",
    "print(tot_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
