{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 设定语言\n",
    "LANGS = [\n",
    "    'java',\n",
    "    'python',\n",
    "    'javascript',\n",
    "    'typescript',\n",
    "    'go'\n",
    "]\n",
    "\n",
    "# 创建目录\n",
    "NECESSARY_DIRS = [f'dataset/{x}' for x in LANGS] \\\n",
    "    + [f'enre_out/{x}' for x in LANGS] \\\n",
    "    + [f'repo/{x}' for x in LANGS]\n",
    "\n",
    "for d in NECESSARY_DIRS:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取各个数据集的仓库 URL，已有忽略\n",
    "\n",
    "# 根据使用修改：存放 CoEdPilot 训练集的目录\n",
    "base_folder = r\"C:\\Users\\aaa\\Desktop\\CoEdPilot-final-phase\\dataset&model\\locator_data\"\n",
    "\n",
    "def commit_url_to_repo_url(commit_url):\n",
    "    return re.search(r\"(.*?)/commit/\\w+$\", commit_url)[1]\n",
    "\n",
    "# 提取 base_folder 中 from_path 数据集的后缀为 lang_suffix 的文件的仓库地址，存放到 out_path\n",
    "def extract_distinct_repo_url(from_path, to_path, lang_suffix):\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, from_path), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".{lang_suffix}\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    distinct_repo_url = list(distinct_repo_url)\n",
    "\n",
    "    with open(to_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(distinct_repo_url, f)\n",
    "\n",
    "# 为指定 lang 提取仓库地址\n",
    "def extract_distinct_repo_url_for_lang(lang, lang_suffix):\n",
    "    extract_distinct_repo_url(f\"{lang}/test.jsonl\", f\"repo_url/{lang}_repo_urls.json\", lang_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {}\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"dev\"]\n",
    "for i in DATASET_TYPES:\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, f\"java/{i}.jsonl\"), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".java\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    sets[i] = distinct_repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_distinct_repo_url_for_lang(\"\", \"js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行对应语言的 ENRE 程序，并制作数据集\n",
    "\n",
    "import make_nn_dataset_for_lang\n",
    "import importlib\n",
    "importlib.reload(make_nn_dataset_for_lang)\n",
    "import json\n",
    "\n",
    "# 为指定语言运行 make_dataset\n",
    "def make_dataset_for_lang(lang):\n",
    "    repos = []\n",
    "    with open(f\"repo_url/{lang}_repo_urls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        repos = json.load(f)\n",
    "\n",
    "    for repo_url in repos:\n",
    "        make_nn_dataset_for_lang.make_dataset(repo_url, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/openzipkin/zipkin. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\zipkin-enre-out\\zipkin-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:28<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 62104, neg_n: 10935\n",
      "Totally 73039 samples\n",
      "dataset length: 73039\n"
     ]
    }
   ],
   "source": [
    "make_nn_dataset_for_lang.make_dataset(\"https://github.com/openzipkin/zipkin\", \"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "with open('dataset/java/zipkin/all.json', 'r') as f:\n",
    "    temp = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"    Endpoint three = Endpoint.newBuilder().serviceName(db).ip(\\\"127.0.0.3\\\").build();\\n\\n    List<Span> trace = asList(\\n      Span.newBuilder().traceId(traceId).id(\\\"10\\\").name(\\\"get\\\")\\n        .kind(Kind.SERVER)\\n\",\n",
      "        \"    Endpoint two = Endpoint.newBuilder().serviceName(backend).ip(\\\"127.0.0.2\\\").build();\\n    Endpoint twoPort3002 = two.toBuilder().port(3002).build();\\n    Endpoint three = Endpoint.newBuilder().serviceName(db).ip(\\\"127.0.0.3\\\").build();\\n\\n    List<Span> trace = asList(\\n      Span.newBuilder().traceId(traceId).id(\\\"10\\\").name(\\\"get\\\")\\n        .kind(Kind.SERVER)\\n        .timestamp(TODAY * 1000L)\\n        .duration(350 * 1000L)\\n        .localEndpoint(one)\\n\"\n",
      "    ],\n",
      "    [\n",
      "        1,\n",
      "        1\n",
      "    ],\n",
      "    [\n",
      "        {\n",
      "            \"src\": 45104,\n",
      "            \"dest\": 45116,\n",
      "            \"type\": \"Set\"\n",
      "        },\n",
      "        {\n",
      "            \"src\": 45104,\n",
      "            \"dest\": 45115,\n",
      "            \"type\": \"Set\"\n",
      "        }\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(random.choice(temp), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/openzipkin/zipkin. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\zipkin-enre-out\\zipkin-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:56<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 18858, neg_n: 37860\n",
      "Totally 56718 samples\n",
      "dataset length: 56718\n",
      "Ignoring https://github.com/signalapp/Signal-Android. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\Signal-Android-enre-out\\Signal-Android-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [01:38<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 36336, neg_n: 73356\n",
      "Totally 109692 samples\n",
      "dataset length: 109692\n",
      "Ignoring https://github.com/oracle/graal. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\graal-enre-out\\graal-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11834/11834 [26:25<00:00,  7.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 304324, neg_n: 612708\n",
      "Totally 917032 samples\n",
      "dataset length: 917032\n",
      "Ignoring https://github.com/scwang90/SmartRefreshLayout. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\SmartRefreshLayout-enre-out\\SmartRefreshLayout-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:04<00:00, 47.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 384, neg_n: 2412\n",
      "Totally 2796 samples\n",
      "dataset length: 2796\n",
      "Ignoring https://github.com/redisson/redisson. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\redisson-enre-out\\redisson-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2482/2482 [01:58<00:00, 20.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 44037, neg_n: 89479\n",
      "Totally 133516 samples\n",
      "dataset length: 133516\n",
      "Ignoring https://github.com/winterbe/java8-tutorial. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\java8-tutorial-enre-out\\java8-tutorial-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:00<00:00, 96.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 315, neg_n: 631\n",
      "Totally 946 samples\n",
      "dataset length: 946\n",
      "Ignoring https://github.com/proxyee-down-org/proxyee-down. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\proxyee-down-enre-out\\proxyee-down-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 34.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 174, neg_n: 610\n",
      "Totally 784 samples\n",
      "dataset length: 784\n",
      "Ignoring https://github.com/spring-projects/spring-framework. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\spring-framework-enre-out\\spring-framework-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7847/7847 [07:39<00:00, 17.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 133153, neg_n: 270576\n",
      "Totally 403729 samples\n",
      "dataset length: 403729\n",
      "Ignoring https://github.com/skylot/jadx. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\jadx-enre-out\\jadx-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1530/1530 [00:37<00:00, 41.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 21642, neg_n: 43466\n",
      "Totally 65108 samples\n",
      "dataset length: 65108\n",
      "Ignoring https://github.com/zxing/zxing. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\zxing-enre-out\\zxing-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [00:19<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 6972, neg_n: 14070\n",
      "Totally 21042 samples\n",
      "dataset length: 21042\n",
      "Ignoring https://github.com/spring-projects/spring-boot. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\spring-boot-enre-out\\spring-boot-out.json existed. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6667/6667 [03:06<00:00, 35.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_n: 61810, neg_n: 125448\n",
      "Totally 187258 samples\n",
      "dataset length: 187258\n",
      "Ignoring https://github.com/termux/termux-app. Repo already cloned\n",
      "ENRE out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\java\\termux-app-enre-out\\termux-app-out.json existed. Skipping...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 377581 column 26 (char 10068035)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmake_dataset_for_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmake_dataset_for_lang\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     12\u001b[0m     repos \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo_url \u001b[38;5;129;01min\u001b[39;00m repos:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mmake_nn_dataset_for_lang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\make_nn_dataset_for_lang.py:162\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(repo_url, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m pos_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    160\u001b[0m neg_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 162\u001b[0m codegraph \u001b[38;5;241m=\u001b[39m \u001b[43mCodeGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_enre_out_file_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m#     print(e.with_traceback(sys.exc_info()[2]))\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m#     print(f\"Cannot build code graph for module {repo_name}. Skipping...\")\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m#     return\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# print(\"Finish building code graph\")\u001b[39;00m\n\u001b[0;32m    170\u001b[0m files \u001b[38;5;241m=\u001b[39m get_files(repo_dir) \u001b[38;5;66;03m# 获取项目下所有文件路径\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\codegraph.py:7\u001b[0m, in \u001b[0;36mCodeGraph.__init__\u001b[1;34m(self, report_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, report_path):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(report_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_node_dict \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# {file_path: {line_idx: [variable_id]}}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_dep_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 377581 column 26 (char 10068035)"
     ]
    }
   ],
   "source": [
    "make_dataset_for_lang(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from run import train_on_single_lang, run_test\n",
    "from itertools import chain\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "\n",
    "def combine_dataset_for_lang(lang):\n",
    "    dataset_dir = f\"dataset/{lang}\"\n",
    "    dataset_of_repos = [f.path for f in os.scandir(dataset_dir) if f.is_dir()]\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        all_data = []\n",
    "        for repo in dataset_of_repos:\n",
    "            with open(f\"{repo}/{dataset_type}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data += json.load(f)\n",
    "        with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(all_data, f)\n",
    "\n",
    "def combine_dataset_for_multiple_lang(langs, out_dir, weights):\n",
    "    if weights is None:\n",
    "        weights = {}\n",
    "    for lang in langs:\n",
    "        if lang not in weights:\n",
    "            weights[lang] = 1.0\n",
    "    print(weights)\n",
    "    \n",
    "    all_data = {x:{y:[] for y in DATASET_TYPES} for x in langs}\n",
    "    \n",
    "    for lang in langs:\n",
    "        dataset_dir = f\"dataset/{lang}\"\n",
    "        for dataset_type in DATASET_TYPES:\n",
    "            data = all_data[lang][dataset_type]\n",
    "            with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                data_to_add = json.load(f)\n",
    "                if weights[lang] < 1.0:\n",
    "                    data_to_add = random.sample(data_to_add, math.floor(len(data_to_add) * weights[lang]))\n",
    "                data += data_to_add\n",
    "\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        data = list(chain.from_iterable([all_data[x][dataset_type] for x in langs]))\n",
    "        random.shuffle(data)\n",
    "        with open(os.path.join(out_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "def train_on_lang_default(lang):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    batch_size = 32\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    train_on_single_lang(model_name, lang, batch_size, train, test, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dataset_for_lang(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java, --train: True, --test True, --batch_size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/478853 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 478853/478853 [05:14<00:00, 1522.07it/s]\n",
      "100%|██████████| 68407/68407 [00:44<00:00, 1548.38it/s]\n",
      "100%|██████████| 136816/136816 [01:29<00:00, 1529.26it/s]\n",
      "Training: 100%|██████████| 14965/14965 [3:24:59<00:00,  1.22it/s, loss=1.01]    \n",
      "Training: 100%|██████████| 14965/14965 [3:18:42<00:00,  1.26it/s, loss=0.0333]  \n",
      "100%|██████████| 2138/2138 [08:24<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9698130308301782, precision: 0.964363916359874, recall: 0.985637696129064, F1: 0.9748847618005132\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9673132866519508, precision: 0.96370441922241, recall: 0.9819635826771653, F1: 0.9727483241925655\n",
      "Validation Loss: 0.10413794356012339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [16:48<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9695284177289206, precision: 0.9641039730511025, recall: 0.9857828588179254, F1: 0.9748229027641059\n",
      "mode: test, A depend by B\n",
      "acc: 0.9670140919190738, precision: 0.9636727791703146, recall: 0.9818661713671245, F1: 0.9726844089893898\n",
      "Test Loss: 0.10475712369907894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14965/14965 [3:00:18<00:00,  1.38it/s, loss=0.0292]   \n",
      "Training: 100%|██████████| 14965/14965 [7:06:37<00:00,  1.71s/it, loss=0.0116]   \n",
      "100%|██████████| 2138/2138 [18:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9746078617685324, precision: 0.9666482844606421, recall: 0.991490826816192, F1: 0.978911969308842\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9740669814492668, precision: 0.9660862474216914, recall: 0.9911417322834646, F1: 0.9784536157601963\n",
      "Validation Loss: 0.08887577650657358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [28:28<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9747178692550579, precision: 0.9668389216993714, recall: 0.9917677378378709, F1: 0.9791446848791429\n",
      "mode: test, A depend by B\n",
      "acc: 0.974045433282657, precision: 0.9663763419080413, recall: 0.991091939977516, F1: 0.97857810756191\n",
      "Test Loss: 0.08917729847261227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 156/14965 [04:53<7:44:48,  1.88s/it, loss=0.0111] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_training_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mrun_training_main\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     22\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --lang: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py:167\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(model_name, lang, batch_size, train, test)\u001b[0m\n\u001b[0;32m    164\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    165\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 167\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    169\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    173\u001b[0m     }, checkpoint_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_lang_default(\"java_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2269/25469 [00:00<00:08, 2676.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 25469/25469 [00:12<00:00, 2097.12it/s]\n",
      "100%|██████████| 796/796 [03:20<00:00,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.7414503906710118, precision: 0.5545750854598934, recall: 0.8766021613470721, F1: 0.6793592053367095\n",
      "mode: test, A depend by B\n",
      "acc: 0.5549491538733362, precision: 0.3475036179450072, recall: 0.4824814768303403, F1: 0.40401703559598295\n",
      "Test Loss: 1.5594558830387029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_test(\"model/java/model_checkpoint_java_23_12_24.bin\", \"python\", 'huggingface/CodeBERTa-small-v1', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1.0, 'java': 0.1}\n"
     ]
    }
   ],
   "source": [
    "combine_dataset_for_multiple_lang([\"python\", \"java\"], \"dataset/python_java\", {\"python\": 1.0, \"java\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java_small, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1607/143655 [00:00<01:00, 2349.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2800 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 143655/143655 [01:00<00:00, 2362.15it/s]\n",
      "100%|██████████| 20522/20522 [00:08<00:00, 2430.77it/s]\n",
      "100%|██████████| 41044/41044 [00:16<00:00, 2437.22it/s]\n",
      "Training: 100%|██████████| 8979/8979 [55:50<00:00,  2.68it/s, loss=0.222]  \n",
      "Training: 100%|██████████| 8979/8979 [55:37<00:00,  2.69it/s, loss=0.135]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9465451710359614, precision: 0.9523265306122449, recall: 0.9578783151326054, F1: 0.955094355069794\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9414774388461163, precision: 0.9486153092067642, recall: 0.9530531845042679, F1: 0.9508290685772774\n",
      "Validation Loss: 0.17346096512682255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:17<00:00,  2.92it/s, loss=0.0194]  \n",
      "Training: 100%|██████████| 8979/8979 [51:19<00:00,  2.92it/s, loss=0.0138]   \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9547802358444596, precision: 0.9612199721242929, recall: 0.9626406108875935, F1: 0.9619297669839186\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9520027287788715, precision: 0.9599934280785345, recall: 0.9591267235718975, F1: 0.9595598801165989\n",
      "Validation Loss: 0.14275865586723518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:19<00:00,  2.92it/s, loss=0.00176]  \n",
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.00417]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9609687164993665, precision: 0.955849358974359, recall: 0.9794728631250513, F1: 0.9675169309379943\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9589708605399084, precision: 0.9541159513132608, recall: 0.9779218647406435, F1: 0.9658722438391699\n",
      "Validation Loss: 0.14923144726442727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.0382]   \n",
      "Training: 100%|██████████| 8979/8979 [51:18<00:00,  2.92it/s, loss=0.00041]  \n",
      "100%|██████████| 1283/1283 [02:33<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9496637754604814, precision: 0.9387498031806015, recall: 0.9790623203875524, F1: 0.9584823761102849\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9481532014423546, precision: 0.9395256916996048, recall: 0.9754596191726855, F1: 0.9571555126036885\n",
      "Validation Loss: 0.19144804556077363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41044/41044 [00:17<00:00, 2406.59it/s]\n",
      "100%|██████████| 2566/2566 [05:06<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9523925543319365, precision: 0.9418713450292397, recall: 0.9811956786613597, F1: 0.9611314449395291\n",
      "mode: test, B depend by A\n",
      "acc: 0.950687067537277, precision: 0.9426489599247855, recall: 0.9772570361044551, F1: 0.9596410767696909\n",
      "Test Loss: 0.17644720438354622\n",
      "--model: huggingface/CodeBERTa-small-v1, --lang: python_java, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected checkpoint file: ./model/java_small\\model_java_small_20231225_225929.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1869/136995 [00:00<01:05, 2054.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 136995/136995 [01:00<00:00, 2261.34it/s]\n",
      "100%|██████████| 19571/19571 [00:08<00:00, 2289.92it/s]\n",
      "100%|██████████| 39150/39150 [00:16<00:00, 2307.88it/s]\n",
      "Training: 100%|██████████| 8563/8563 [53:02<00:00,  2.69it/s, loss=0.47]   \n",
      "Training: 100%|██████████| 8563/8563 [53:05<00:00,  2.69it/s, loss=0.128]  \n",
      "100%|██████████| 1224/1224 [02:26<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9508456389555976, precision: 0.9313440077632217, recall: 0.9509536784741145, F1: 0.9410466968991299\n",
      "mode: dev, B depend by A\n",
      "acc: 0.946349190128251, precision: 0.9151780432982373, recall: 0.9587309455942497, F1: 0.9364483718678126\n",
      "Validation Loss: 0.18471033477948773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8563/8563 [48:57<00:00,  2.91it/s, loss=0.00892] \n",
      "Training: 100%|██████████| 8563/8563 [49:00<00:00,  2.91it/s, loss=0.0244]   \n",
      "100%|██████████| 1224/1224 [02:26<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9597363445914874, precision: 0.9399758454106281, recall: 0.9639583849393114, F1: 0.9518160694631284\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9585100403658474, precision: 0.9366951498375256, recall: 0.9645557070268931, F1: 0.9504212968616437\n",
      "Validation Loss: 0.12643623364139334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8563/8563 [48:59<00:00,  2.91it/s, loss=0.000844] \n",
      "Training: 100%|██████████| 8563/8563 [50:56<00:00,  2.80it/s, loss=0.000804] \n",
      "100%|██████████| 1224/1224 [02:33<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9562107199427725, precision: 0.9348114230630197, recall: 0.9608620262571216, F1: 0.9476577291882978\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9549844157171324, precision: 0.9301101005265677, recall: 0.9631924649894659, F1: 0.9463622526636225\n",
      "Validation Loss: 0.1652732740799395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 793/8563 [04:43<46:21,  2.79it/s, loss=0.00348]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aaa\\AppData\\Local\\Temp\\ipykernel_31864\\2672801796.py\", line 9, in <module>\n",
      "    train_on_lang_from_checkpoint(\"python_java\", \"./model/java_small/*.bin\", epoch=8, batch_size=16)\n",
      "  File \"C:\\Users\\aaa\\AppData\\Local\\Temp\\ipykernel_31864\\2672801796.py\", line 6, in train_on_lang_from_checkpoint\n",
      "    train_on_single_lang(model_name, lang, batch_size, train, test, checkpoint_path, epoch)\n",
      "  File \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py\", line 174, in train_on_single_lang\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\aaa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def train_on_lang_from_checkpoint(lang, checkpoint_path=None, epoch=10, batch_size=32):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    train_on_single_lang(model_name, lang, batch_size, train, test, checkpoint_path, epoch)\n",
    "\n",
    "train_on_lang_from_checkpoint(\"java_new_small\", epoch=6, batch_size=16)\n",
    "# train_on_lang_from_checkpoint(\"python_java\", \"./model/java_small/*.bin\", epoch=8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java_small, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected checkpoint file: ./model/java\\model_checkpoint_java_23_12_24.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1656/143655 [00:00<00:59, 2404.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2800 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 143655/143655 [01:02<00:00, 2286.07it/s]\n",
      "100%|██████████| 20522/20522 [00:08<00:00, 2291.92it/s]\n",
      "100%|██████████| 41044/41044 [00:17<00:00, 2328.42it/s]\n",
      "Training:   1%|          | 67/8979 [00:29<1:04:27,  2.30it/s, loss=0.82] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_on_lang_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava_small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/java/*.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mtrain_on_lang_from_checkpoint\u001b[1;34m(lang, checkpoint_path, epoch, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --lang: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_on_single_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py:174\u001b[0m, in \u001b[0;36mtrain_on_single_lang\u001b[1;34m(model_name, lang, batch_size, train, test, checkpoint_path, epoch_num)\u001b[0m\n\u001b[0;32m    171\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    172\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 174\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    176\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    180\u001b[0m     }, save_checkpoint_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_on_lang_from_checkpoint(\"java_small\", \"./model/java/*.bin\", epoch=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/java/'\n",
    "for dataset_type in DATASET_TYPES:\n",
    "    with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        data_to_add = json.load(f)\n",
    "        data_to_add = random.sample(data_to_add, math.floor(len(data_to_add) * 0.3))\n",
    "        with open('dataset/java_small/' + dataset_type + '.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(data_to_add, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127310\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "tot_len = 0\n",
    "for i in DATASET_TYPES:\n",
    "    with open(f\"./dataset/python/{i}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        tot_len += len(json.load(f))\n",
    "print(tot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using graal for training\n",
      "using jadx for training\n",
      "using java8-tutorial for training\n",
      "using proxyee-down for training\n",
      "using redisson for repo type ['redisson', 'Signal-Android']\n",
      "using Signal-Android for repo type ['redisson', 'Signal-Android']\n",
      "using SmartRefreshLayout for training\n",
      "using spring-boot for repo type ['spring-boot', 'spring-framework']\n",
      "using spring-framework for repo type ['spring-boot', 'spring-framework']\n",
      "using zipkin for training\n",
      "using zxing for training\n",
      "train dataset length: 1064426\n",
      "valid dataset length: 243208\n",
      "test dataset length: 590987\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "ENRE_OUT_PATH = \"dataset/java\"\n",
    "\n",
    "VALID_REPOS = ['redisson', 'Signal-Android']\n",
    "TEST_REPOS = ['spring-boot', 'spring-framework']\n",
    "\n",
    "datasets = {\n",
    "    'train': [],\n",
    "    'valid': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "def filter_data_to(i, repo_set, dataset):\n",
    "    if i in repo_set:\n",
    "        print(f'using {i} for repo type {repo_set}')\n",
    "        with open(os.path.join(ENRE_OUT_PATH, i, \"all.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset += data\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for i in os.listdir(ENRE_OUT_PATH):\n",
    "    if not os.path.isdir(os.path.join(ENRE_OUT_PATH, i)):\n",
    "        continue\n",
    "    if not (filter_data_to(i, VALID_REPOS, datasets['valid']) or filter_data_to(i, TEST_REPOS, datasets['test'])):\n",
    "        print(f'using {i} for training')\n",
    "        with open(os.path.join(ENRE_OUT_PATH, i, \"all.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            datasets['train'] += data\n",
    "\n",
    "for dataset_type in 'train', 'valid', 'test':\n",
    "    print(f'{dataset_type} dataset length: {len(datasets[dataset_type])}')\n",
    "    random.shuffle(datasets[dataset_type])\n",
    "    with open(os.path.join(ENRE_OUT_PATH, f\"{dataset_type}.json\"), 'w') as f:\n",
    "        json.dump(datasets[dataset_type], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_type in 'train', 'valid', 'test':\n",
    "    with open(os.path.join(ENRE_OUT_PATH, f\"{dataset_type}.json\"), 'r', encoding=\"utf-8\") as f:\n",
    "        large_dataset = json.load(f)\n",
    "    small_dataset = random.sample(large_dataset, int(len(large_dataset)*0.1))\n",
    "    with open(os.path.join(ENRE_OUT_PATH, '../java_new_small', f\"{dataset_type}.json\"), 'w') as f:\n",
    "        json.dump(small_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java_new_small, --train: True, --test True, --batch_size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/106442 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (756 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 106442/106442 [00:52<00:00, 2028.32it/s]\n",
      "100%|██████████| 24320/24320 [00:10<00:00, 2265.45it/s]\n",
      "100%|██████████| 59098/59098 [00:25<00:00, 2292.85it/s]\n",
      "Training: 100%|██████████| 6653/6653 [43:14<00:00,  2.56it/s, loss=0.169]  \n",
      "Training: 100%|██████████| 6653/6653 [43:10<00:00,  2.57it/s, loss=0.186]  \n",
      "100%|██████████| 1520/1520 [03:02<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.941735197368421, precision: 0.8371749044743005, recall: 0.9860627177700348, F1: 0.9055396306912873\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9868009868421053, precision: 0.9624293785310735, recall: 0.9919930120832727, F1: 0.9769875976772529\n",
      "Validation Loss: 0.18777880140972372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6653/6653 [39:09<00:00,  2.83it/s, loss=0.000283]\n",
      "Training: 100%|██████████| 6653/6653 [38:14<00:00,  2.90it/s, loss=0.000218] \n",
      "100%|██████████| 1520/1520 [03:00<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9048519736842106, precision: 0.7516505281690141, recall: 0.9917247386759582, F1: 0.8551577366049075\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9847861842105263, precision: 0.958904109589041, recall: 0.9884990537196099, F1: 0.9734767025089605\n",
      "Validation Loss: 0.24292665464707552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6653/6653 [38:50<00:00,  2.85it/s, loss=0.00034]  \n",
      "Training: 100%|██████████| 6653/6653 [39:14<00:00,  2.83it/s, loss=0.0347]  \n",
      "100%|██████████| 1520/1520 [03:11<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9050986842105263, precision: 0.7510964912280702, recall: 0.9944831591173054, F1: 0.8558220889555223\n",
      "mode: dev, B depend by A\n",
      "acc: 0.9850740131578948, precision: 0.9575246132208157, recall: 0.991119522492357, F1: 0.9740324772873596\n",
      "Validation Loss: 0.21279558275588753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59098/59098 [00:26<00:00, 2268.38it/s]\n",
      "100%|██████████| 3694/3694 [07:52<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.99076110866696, precision: 0.9900773963087914, recall: 0.974223784417106, F1: 0.9820866141732284\n",
      "mode: test, B depend by A\n",
      "acc: 0.9935700023689465, precision: 0.9889610389610389, recall: 0.9863989637305699, F1: 0.9876783398184176\n",
      "Test Loss: 0.02648400789081337\n"
     ]
    }
   ],
   "source": [
    "def train_on_lang_from_checkpoint(lang, checkpoint_path=None, epoch=10, batch_size=32):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    train_on_single_lang(model_name, lang, batch_size, train, test, checkpoint_path, epoch)\n",
    "\n",
    "train_on_lang_from_checkpoint(\"java_new_small\", epoch=6, batch_size=16)\n",
    "# train_on_lang_from_checkpoint(\"python_java\", \"./model/java_small/*.bin\", epoch=8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cp = torch.load('model/java_new_small/model_java_new_small_20240103_135049_epoch5.bin')\n",
    "torch.save({'model_state_dict': cp['model_state_dict']}, 'model_java_new_small_20240103_135049_epoch5.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
