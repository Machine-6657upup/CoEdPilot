{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 设定语言\n",
    "LANGS = [\n",
    "    'java',\n",
    "    'python',\n",
    "    'javascript',\n",
    "    'typescript',\n",
    "    'go'\n",
    "]\n",
    "\n",
    "# 创建目录\n",
    "NECESSARY_DIRS = [f'dataset/{x}' for x in LANGS] \\\n",
    "    + [f'enre_out/{x}' for x in LANGS] \\\n",
    "    + [f'repo/{x}' for x in LANGS]\n",
    "\n",
    "for d in NECESSARY_DIRS:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取各个数据集的仓库 URL，已有忽略\n",
    "\n",
    "# 根据使用修改：存放 CoEdPilot 训练集的目录\n",
    "base_folder = r\"C:\\Users\\aaa\\Desktop\\CoEdPilot-final-phase\\dataset&model\\locator_data\"\n",
    "\n",
    "def commit_url_to_repo_url(commit_url):\n",
    "    return re.search(r\"(.*?)/commit/\\w+$\", commit_url)[1]\n",
    "\n",
    "# 提取 base_folder 中 from_path 数据集的后缀为 lang_suffix 的文件的仓库地址，存放到 out_path\n",
    "def extract_distinct_repo_url(from_path, to_path, lang_suffix):\n",
    "    distinct_repo_url = set()\n",
    "    with open(os.path.join(base_folder, from_path), 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            if json_data[\"file_name\"].endswith(f\".{lang_suffix}\"):\n",
    "                distinct_repo_url.add(commit_url_to_repo_url(json_data[\"html_url\"]))\n",
    "    distinct_repo_url = list(distinct_repo_url)\n",
    "\n",
    "    with open(to_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(distinct_repo_url, f)\n",
    "\n",
    "# 为指定 lang 提取仓库地址\n",
    "def extract_distinct_repo_url_for_lang(lang, lang_suffix):\n",
    "    extract_distinct_repo_url(f\"{lang}/test.jsonl\", f\"repo_url/{lang}_repo_urls.json\", lang_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_distinct_repo_url_for_lang(\"javascript\", \"js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行对应语言的 ENRE 程序，并制作数据集\n",
    "\n",
    "import make_nn_dataset_for_lang\n",
    "import importlib\n",
    "importlib.reload(make_nn_dataset_for_lang)\n",
    "import json\n",
    "\n",
    "# 为指定语言运行 make_dataset\n",
    "def make_dataset_for_lang(lang):\n",
    "    repos = []\n",
    "    with open(f\"repo_url/{lang}_repo_urls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        repos = json.load(f)\n",
    "\n",
    "    for repo_url in repos:\n",
    "        make_nn_dataset_for_lang.make_dataset(repo_url, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vercel/next.js. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" && node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\typescript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\next.js -v\"\n"
     ]
    }
   ],
   "source": [
    "make_nn_dataset_for_lang.make_dataset(\"https://github.com/vercel/next.js\", \"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring https://github.com/vuejs/vue-cli. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\vue-cli -o vue-cli-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"vue-cli\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\vue-cli-report-enre.json\n",
      "Ignoring https://github.com/yarnpkg/yarn. Repo already cloned\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\" &&node c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\enre-ts-0.0.1-gamma.js -i c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\\yarn -o yarn-report-enre.json\"\n",
      "ENRE javascript: generated dep for \"yarn\" at c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "ENRE cannot generated out file c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\enre_out\\javascript\\yarn-report-enre.json\n",
      "Executing command: \"cd \"c:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\repo\\javascript\" && git clone \"https://github.com/vercel/next.js.git\"\"\n"
     ]
    }
   ],
   "source": [
    "make_dataset_for_lang(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from run import main\n",
    "\n",
    "DATASET_TYPES = [\"test\", \"train\", \"valid\"]\n",
    "\n",
    "def combine_dataset_for_lang(lang):\n",
    "    dataset_dir = f\"dataset/{lang}\"\n",
    "    dataset_of_repos = [f.path for f in os.scandir(dataset_dir) if f.is_dir()]\n",
    "    for dataset_type in DATASET_TYPES:\n",
    "        all_data = []\n",
    "        for repo in dataset_of_repos:\n",
    "            with open(f\"{repo}/{dataset_type}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data += json.load(f)\n",
    "        with open(os.path.join(dataset_dir, f\"{dataset_type}.json\"), \"w\") as f:\n",
    "            json.dump(all_data, f)\n",
    "        \n",
    "def run_training_main(lang):\n",
    "    model_name = 'huggingface/CodeBERTa-small-v1'\n",
    "    batch_size = 32\n",
    "    train = True\n",
    "    test = True\n",
    "    print(f'--model: {model_name}, --lang: {lang}, --train: {train}, --test {test}, --batch_size: {batch_size}')\n",
    "    main(model_name, lang, batch_size, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dataset_for_lang(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model: huggingface/CodeBERTa-small-v1, --lang: java, --train: True, --test True, --batch_size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/478853 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 478853/478853 [05:14<00:00, 1522.07it/s]\n",
      "100%|██████████| 68407/68407 [00:44<00:00, 1548.38it/s]\n",
      "100%|██████████| 136816/136816 [01:29<00:00, 1529.26it/s]\n",
      "Training: 100%|██████████| 14965/14965 [3:24:59<00:00,  1.22it/s, loss=1.01]    \n",
      "Training: 100%|██████████| 14965/14965 [3:18:42<00:00,  1.26it/s, loss=0.0333]  \n",
      "100%|██████████| 2138/2138 [08:24<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9698130308301782, precision: 0.964363916359874, recall: 0.985637696129064, F1: 0.9748847618005132\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9673132866519508, precision: 0.96370441922241, recall: 0.9819635826771653, F1: 0.9727483241925655\n",
      "Validation Loss: 0.10413794356012339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [16:48<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9695284177289206, precision: 0.9641039730511025, recall: 0.9857828588179254, F1: 0.9748229027641059\n",
      "mode: test, A depend by B\n",
      "acc: 0.9670140919190738, precision: 0.9636727791703146, recall: 0.9818661713671245, F1: 0.9726844089893898\n",
      "Test Loss: 0.10475712369907894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14965/14965 [3:00:18<00:00,  1.38it/s, loss=0.0292]   \n",
      "Training: 100%|██████████| 14965/14965 [7:06:37<00:00,  1.71s/it, loss=0.0116]   \n",
      "100%|██████████| 2138/2138 [18:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: dev, A depend on B\n",
      "acc: 0.9746078617685324, precision: 0.9666482844606421, recall: 0.991490826816192, F1: 0.978911969308842\n",
      "mode: dev, A depend by B\n",
      "acc: 0.9740669814492668, precision: 0.9660862474216914, recall: 0.9911417322834646, F1: 0.9784536157601963\n",
      "Validation Loss: 0.08887577650657358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4276/4276 [28:28<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test, A depend on B\n",
      "acc: 0.9747178692550579, precision: 0.9668389216993714, recall: 0.9917677378378709, F1: 0.9791446848791429\n",
      "mode: test, A depend by B\n",
      "acc: 0.974045433282657, precision: 0.9663763419080413, recall: 0.991091939977516, F1: 0.97857810756191\n",
      "Test Loss: 0.08917729847261227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 156/14965 [04:53<7:44:48,  1.88s/it, loss=0.0111] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_training_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mrun_training_main\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     22\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --lang: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, --batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaa\\Desktop\\edit-pilot-model\\dependency_analyzer\\run.py:167\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(model_name, lang, batch_size, train, test)\u001b[0m\n\u001b[0;32m    164\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    165\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 167\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    169\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    173\u001b[0m     }, checkpoint_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_training_main(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
